# LPCVC-Track-3

This repository contains the sample solution for LPCVC 2026 Track 3: AI Generated Images Detection. It also contains instructions for preparing the necessary files for submission.

Our approach is based on Qwen2-VL-2B-Instruct, following the model preparation tutorial from [QPM](https://qpm.qualcomm.com/#/main/tools/details/Tutorial_for_Qwen2_VL_2b_IoT).

## Sample Solution

Download the complete sample solution zip [here](https://purdue0-my.sharepoint.com/:u:/g/personal/su431_purdue_edu/IQATLC6t7JqWQJTowBKKpOXHAffuaUnrlexCru5idbJklxA?e=YjZ7Qu). Several files aren't included in the GitHub repository due to file size limits. The zip file has size 1.8 GB, fully unzipped it has size 2.4 GB

---

## 0. Download the Tutorial

Download ["Tutorial for Qwen2_VL_2b (IoT)"](https://qpm.qualcomm.com/#/main/tools/details/Tutorial_for_Qwen2_VL_2b_IoT) from [Qualcomm Package Manager (QPM)](https://qpm.qualcomm.com/#/main/home).

## 1. Getting started

Starting with the `README.md` in `example1`, work sequentially through the `example1` and `example2` directories to generate the files.
- example1
	- PyTorch Model optimization and export using AIMET
- example2
	- Preparation and conversion of ONNX model to Qualcomm NN 

## 2. Preparing files for submission

Once all files have been generated, we will prepare the necessary files needed for submission.

### Files needed for submission:
```
submission_files
├── ar*-ar*-cl*
│   └── weight_sharing_model_1_of_1.serialized.bin
├── embedding_weights*.raw
├── inputs.json
├── mask.raw
├── position_ids_cos.raw
├── position_ids_sin.raw
├── serialized_binaries
│   └── veg.serialized.bin
└── tokenizer.json
```

All files, excluding `inputs.json`, should have been generated by running `example1` and `example2` from the tutorial. We will provide additional instructions for generating `inputs.json` below.

The `ar*-ar*-cl*` folder can be named anything within the pattern. This gives contestants more flexibility. In this sample solution, the complete folder name is `ar128-ar1-cl2048` and only contains 1 file. 

The `embedding_weights*.raw` file can be named anything within the pattern. This gives contestants more flexibility. In this sample solution, the complete file name is `embedding_weights_151936x1536.raw`

### The needed files/folders can be found in the following locations:
```
Tutorial_for_Qwen2_VL_2b_IoT
├── example1
│   ├── Example1A
│   │   └── output_dir
│   │       └── veg_exports
│   │           ├── mask.raw
│   │           ├── position_ids_cos.raw
│   │           └── position_ids_sin.raw
│   └── Example1B
│       └── output_dir
│           ├── embedding_weights_151936x1536.raw
│           └── tokenizer
│               └── tokenizer.json
├── example2
│   ├── Example2A
│   │   └── host_linux
│   │       └── exports
│   │           └── serialized_binaries
│   │               └── veg.serialized.bin
│   └── Example2B
│       └── host_linux
│           └── assets
│               └── artifacts
│                   └── ar128-ar1-cl2048
│                       └── weight_sharing_model_1_of_1.serialized.bin
└── example3
    └── qnn_model_execution.ipynb
        └─> Extract several parameters into `inputs.json`
```

## Creating `inputs.json`

This file contains the parameters needed to run model inference. In the tutorial, it was hard-coded within the notebooks for Qwen2-vl-2B. To support multiple models beyond Qwen2-vl-2B in our inference script, the contestant must provide additional parameters.

`inputs.json` should be a JSON file containing the following parameters extracted from `qnn_model_execution.ipynb` in `example3`:
### `inputs.json`    
```json
{
  "qwen_vl_processor": "Qwen/Qwen2-VL-2B-Instruct",
  "llm_config": "Qwen/Qwen2-VL-2B-Instruct",
  "data_preprocess_inp_h": 342,
  "data_preprocess_inp_w": 512,
  "run_veg_n_tokens": 216,
  "run_veg_embedding_dim": 1536,
  "genie_config": { ... (redacted for length) ... }
}
```
### Table of parameters 
| Parameter | Description | Location within `qnn_model_execution.ipynb` |
|-----------|-------------|----------|
| qwen_vl_processor     | Name of the qwen vl processor |`qwen2_vl_processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")`|
| llm_config            | Name of the llm | `llm_config = AutoConfig.from_pretrained("Qwen/Qwen2-VL-2B-Instruct", trust_remote_code=True)` |
| data_preprocess_inp_h | Image height taken by the `data_process` function | `inputs = data_preprocess(qwen2_vl_processor, image_file, 342, 512, prompt)` |
| data_preprocess_inp_w | Image width taken by the `data_process` function | `inputs = data_preprocess(qwen2_vl_processor, image_file, 342, 512, prompt)` |
| run_veg_n_tokens      | Second shape of the output in the `run_veg` function | `output_data = output_data.reshape((1, 216, 1536))` |
| run_veg_embedding_dim | Third shape of the output in the `run_veg` function | `output_data = output_data.reshape((1, 216, 1536))` |
| genie_config          | The entirety of the json found in the code cell under "Creating Genie Config JSON". Do not worry about the paths, we will set them for you.  | `genie_config = { ...... }` |
---

## 3. Uploading your submission

Place all necessary submission files mentioned above into a folder named with your team name. 
The folder should contain:
- 2 subdirectories
- 2 .json files
- 4 .raw files

Zip the folder and also name the zip file with your team name. Your final submission should look like:

```
team_name.zip/
└── team_name/
    ├── ar*-ar*-cl*/
    ├── serialized_binaries/
    ├── embedding_weights*.raw
    ├── inputs.json
    ├── mask.raw
    ├── position_ids_cos.raw
    ├── position_ids_sin.raw
    └── tokenizer.json
```

## 4. Running Inference

If you have a Snapdragon 8 Gen 5 Android device, here are the steps to run your model on it. Place all your files within the `contestant_uploads` folder. It should not be your team_name folder or zip.

Run the following command within the conda environment:

`python inference_script.py`

Files will be pulled back into the `Host_Outputs` folder.